---
title: "Model Stacking, Generally"
output: html_document
---

## Fitting Individual Models

Setting up Data:

* Split into testing/training (`rsample::initial_split()`, `rsample::training()`, `rsample::testing()`)  
* Cross validation (`rsample::vfold_cv()`)  
* Pre-processing (`recipes::recipe()`, `recipes::step_*()`)  

Create an object to supply to `tune::tune_grid()` calls that ensures predictions are saved. (`tune::control_grid(save_pred = TRUE)`)

Set metric. (`yardstick::metric_set()`)

Build constituent models. For each:

* Extend pre-processing, if necessary (more `recipes::step_*()`)  
* Set a model specification (with parsnip or rules)  
* Choose a package (`parsnip::set_engine()`)  
* Possibly: create a workflow (`workflows::workflow()`, `workflows::add_recipe()`, `workflows::add_model()`), tune hyperparameters (`tune::tune_grid()`) (possibly using a workflow)  

## Stacking / Ensembling

Ensembling refers to techniques that make use of multiple learning algorithms to improve model performance. Stacking is a subset of ensembling techniques where outputs from many different models are combined to make a final prediction (where the function/weights used to do the combining is/are arbitrary and can be learned).

Create "stacked data" (i.e. a dataframe with columns `y`, `y_hat`, `y_hat2`, etc.)

* Currently uses the `get_best_pred()` function several times. 

```{r, eval = FALSE}
stack_data <-
  ames_train %>%
  select(Sale_Price) %>%
  add_rowindex() %>%
  full_join(get_best_pred(cubist_res, "cubist"), by = ".row") %>%
  full_join(get_best_pred(glmnet_res, "glmnet"), by = ".row") %>%
  full_join(get_best_pred(mars_res, "mars"), by = ".row") %>%
  dplyr::select(-.row)
```

What about a `stack_predictions(...)` function that takes in named resampling results, like

```{r, eval = FALSE}
stack_data <- 
  ames_train %>%
  stack_predictions(cubist = cubist_res, 
                    glmnet = glmnet_res,
                    mars = mars_res)
```

`get_all_pred()` is an alternative to `get_best_pred()` where, instead of selecting the best sub-model and then calling `collect_predictions()` on it, the function collects the predictions from all of the sub-models (i.e. regardless of choice of hyperparameter). See the `stack_predictions.R` file for a modified spec.

Create stacking loadings/coefficients based on model regularization
   + Create a stacked model specification
   + Use tuning to determine how much to weight each model
   + Fit using the finalized model and stacked data
   
Collate resampling results:
   
```{r, eval = FALSE}
res <-
  ens_stacked() %>%
  add_member(fit = cubist_fit, results = cubist_res) %>%
  add_member(fit = glmnet_fit, results = glmnet_res) %>%
  add_member(fit = mars_fit,   results = mars_res)
```

Possibly `stack_init()`? Or `ens_init()`? How important is it to have many functions start with the same prefix?

Add the coefficients to the collated resamples: 

* `evaluate(res, model = "glmnet", parameters, resampling)`
* `evaluate()` feels sort of general, `refit()` conflicts with `lme4`. Maybe something like `stack_eval`?
* How much is this function actually doing, though? Have the coefficients been calculated yet? Why is this coming before `fit()`?

`fit()` the model

* Make use of the `generics::fit()` generic

`predict()` new data

* Should have a `type` argument for classification methods




