At the highest level, ensembles are formed from _model definitions_. In this package, model definitions are an instance of a minimal workflow, containing a _model specification_ (as defined in the parsnip package) and, optionally, a _preprocessor_ (as defined in the recipes package). Model definitions specify the form of candidate ensemble members.

\\figure{model_defs.png}{options: width=500}

To be used in the same ensemble, each of these model definitions must share the same _resample_. This rsample `rset` object, when paired with the model definitions, can be used to generate the tuning/fitting results objects for the candidate _ensemble members_ with tune.

\\figure{submodels.png}{options: width=500}

The package will sometimes refer to _sub-models_. An ensemble member is a sub-model that has actually been selected (and possibly trained) for use in the ensemble (via nonzero stacking coefficients, usually) that is not regarded as resulting from a specific model definition, where-as a sub-model is an untrained candidate ensemble member.

Sub-models first come together in a `data_stack` object through the `stack_add()` function. Principally, these objects are just [tibbles](https://tibble.tidyverse.org/), where the first column gives the true outcome in the assessment set, and the remaining columns give the predictions from each candidate ensemble member. (When the outcome is numeric, there's only one column per candidate ensemble member. Classification requires as many columns per candidate as there are levels in the outcome variable.) They also bring along a few extra attributes to keep track of model definitions.

\\figure{data_stack.png}{options: width=500}

Then, the data stack can be evaluated using `stack_blend()` to determine to how best to combine the outputs from each of the candidate member models.  

Note that the fitting process is not sensitive to model definition membership. That is, while fitting an ensemble from a stack, the components are regarded as candidate ensemble members rather than as sub-models.   

The outputs of each member are likely highly correlated. Thus, depending on the degree of regularization you choose, the coefficients for the inputs of (possibly) many of the members will zero out—their predictions will have no influence on the final output, and those terms will thus be thrown out.  

\\figure{coefs.png}{options: width=500}

These stacking coefficients decide then which sub-models will be ensemble members—sub-models with non-zero stacking coefficients are then fitted, altogether making up a `model_stack` object.

\\figure{class_model_stack.png}{options: width=500}

This model stack object, outputted from `stack_fit()`, is ready to predict on new data!

At a high level, the process follows these steps:

\\figure{outline.png}{options: width=500}

The API for the package closely mirrors these ideas. See the `basics` vignette for an example of how this grammar is implemented!
